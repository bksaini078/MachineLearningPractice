{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bksaini078/MachineLearningPractice/blob/main/BiLSTM_Attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA482cSwxo5d"
   },
   "source": [
    "Author: Bhupender Kumar Saini\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH_jzqgtaKZE"
   },
   "source": [
    "# Declaring Libraries \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOCj2bQfiLH8"
   },
   "outputs": [],
   "source": [
    "#please install contractions \n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "# import contractions\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim import utils \n",
    "from gensim.test.utils import get_tmpfile\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# nlp = en_core_web_sm.load()\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# porter=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dlQXTXLez5Yl"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#this is to enable eager execution\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "path = '/content/drive/MyDrive/Semi_super/ExperimentFolds/fakehealth/preprocessed/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Lpuz0gk_DW"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7XLM4EqlCHu"
   },
   "outputs": [],
   "source": [
    "full_article_temp=[]\n",
    "for i in range(5):\n",
    "    # print('-------------FOLD Number:---------------',i)\n",
    "    x_train= np.load(path+'train_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    x_val= np.load(path+'dev_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    x_test= np.load(path+'test_x.npy', allow_pickle=True)\n",
    "    full_article_temp= np.hstack((x_train, x_val, full_article_temp, x_test))\n",
    "\n",
    "def tokenization(x_train, x_val,  x_test, x_unlabel, maxlen ):\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    import numpy as np\n",
    "    tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ',\n",
    "                          char_level=False, oov_token=None, document_count=0)\n",
    "    # full_article = np.hstack((x_train, x_test, x_unlabel))\n",
    "    full_article= np.hstack(( x_unlabel, full_article_temp))\n",
    "    tokenizer.fit_on_texts(full_article)\n",
    "    x_train_token = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test_token = tokenizer.texts_to_sequences(x_test)\n",
    "    x_val_token = tokenizer.texts_to_sequences(x_val)\n",
    "    x_unlabel_token = tokenizer.texts_to_sequences(x_unlabel)\n",
    "\n",
    "    x_train_seq = sequence.pad_sequences(x_train_token, maxlen=maxlen,padding='post')\n",
    "    x_test_seq = sequence.pad_sequences(x_test_token, maxlen=maxlen,padding='post')\n",
    "    x_val_seq = sequence.pad_sequences(x_val_token, maxlen=maxlen,padding='post')\n",
    "    x_unlabel_tar= sequence.pad_sequences(x_unlabel_token, maxlen=maxlen,padding='post')\n",
    "    # defining vocalbury size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "    x_train = x_train_seq\n",
    "    x_test = x_test_seq\n",
    "    return x_train,x_val_seq, x_test, x_unlabel_tar, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AozvbK25RZ6m"
   },
   "source": [
    "# Noise Creator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX-fC3dxbzJ6"
   },
   "outputs": [],
   "source": [
    "def instant_noise(x_train, y_train, x_unlabel, n_ratio ):\n",
    "    '''this function introduce noise in the training data for mean teacher model , \n",
    "    this function is used in calculating classification cost, user have to provide \n",
    "    amount of noise, want to add(ratio) in train data and test train split ratio too'''\n",
    "    #amount of noise need to add in x_train data \n",
    "    # print('In noise function:e', np.shape(x_train))\n",
    "    noise=int(np.shape(x_train)[0]*n_ratio)\n",
    " \n",
    "    # taking column of x_train, need it later \n",
    "    x_column = np.shape(x_train)[1]\n",
    "\n",
    "    if noise <= int(np.shape(x_unlabel)[0]):\n",
    "\n",
    "        #taking number of noise from unlabel data \n",
    "        ratio_noise = x_unlabel[:noise]\n",
    "\n",
    "        # creating -1 label for noise data \n",
    "        y_unlabel=np.full((np.shape(ratio_noise)[0], 1), -1)\n",
    "\n",
    "        # adding noise in train data \n",
    "        x = np.append(x_train, ratio_noise, axis=0)\n",
    "        # print(np.shape(x))\n",
    "        y = np.append(y_train, y_unlabel, axis=0)\n",
    "        x = np.append(x,y, axis=1)\n",
    "        x= x[noise:]\n",
    "        row = np.shape(x)[0]\n",
    "        # print('In noise function:l', np.shape(x_train))\n",
    "\n",
    "        # shufflin data \n",
    "        x =np.random.permutation(x)\n",
    "        # print(np.shape(x))\n",
    "        \n",
    "        #seperating label from x \n",
    "        y_train_n=np.reshape(x[:,x_column],(row,1))\n",
    "        x_train_n=x[0:len(x),0:x_column]\n",
    "        # y_train_n= np.reshape(y[:len(x),0],(train_split,1))\n",
    "\n",
    "        \n",
    "    else :\n",
    "        print('error: Insufficient unlabel data available !')\n",
    "\n",
    "    return x_train_n, y_train_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swtc3CtsvXH5"
   },
   "source": [
    "# Model Declaration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFjsH6yzgB4V"
   },
   "outputs": [],
   "source": [
    "def BiLstmModel_attention(maxlen, vocab_size):\n",
    "  import keras\n",
    "  tf.keras.backend.clear_session()\n",
    "  inputs = keras.Input(shape=(maxlen,))\n",
    "  x = Embedding(vocab_size, 128, input_length=None)(inputs)\n",
    "  x = Bidirectional(LSTM(128,return_sequences= True))(x)\n",
    "  x = tf.keras.layers.Attention()([x,x])\n",
    "  x= Flatten()(x)\n",
    "  x = Dense(64)(x)\n",
    "  x = Dense(1, activation='sigmoid')(x)\n",
    "  return Model(inputs,x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrpTHBDBb4Uv"
   },
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR5z1R9C2big"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Function to create confusion matrix \n",
    "def Confusion_matrix(model,x_test,y_true, threshold, caption='Confusion matrix'):\n",
    "    '''this function will create confusion matrix with predicted value and true label'''\n",
    "    y_hat= model.predict(x_test)\n",
    "    y_pred=(np.greater_equal(y_hat,threshold)).astype(int)\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    # print(cm)\n",
    "    # calculating recall , precision and f1 score \n",
    "    tp_and_fp=np.sum(cm[:,1])\n",
    "    tn_and_fp=np.sum(cm[0,:])\n",
    "    tp_and_fn = np.sum(cm[1, : ])\n",
    "    tp_and_tn= np.trace(cm)\n",
    "    tp=(tp_and_fp-tn_and_fp+tp_and_tn)/2\n",
    "    '''handling with divide by zero is pending'''\n",
    "    #TODO: handling of divide by zero \n",
    "    precision=tp/tp_and_fp \n",
    "    recall = tp/tp_and_fn\n",
    "    accuracy= np.trace(cm)/np.sum(cm)\n",
    "    # f1_score=sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    f1_score= (2*precision*recall)/(precision+recall)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('f1 Score:', f1_score)\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    # plt.show()\n",
    "    return cm, accuracy, precision, recall, f1_score\n",
    "def prec_rec_f1score(y_true,x_test,model):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn import metrics\n",
    "    bce = tf.keras.losses.BinaryCrossentropy()\n",
    "    y_hat= model.predict(x_test)\n",
    "    y_pred=(np.greater_equal(y_hat,0.51)).astype(int)\n",
    "\n",
    "    pr_re_f1score_perclass= precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "    pr_re_f1score_average=precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    precision=precision_score(y_true,y_pred,average=None)\n",
    "    recall = recall_score(y_true,y_pred,average=None)\n",
    "    accuracy= accuracy_score(y_true,y_pred)\n",
    "    f1_score=f1_score(y_true,y_pred)\n",
    "    #per class\n",
    "    precision_true=pr_re_f1score_perclass[0][1]\n",
    "    precision_fake=pr_re_f1score_perclass[0][0]\n",
    "    recall_true=pr_re_f1score_perclass[1][1]\n",
    "    recall_fake=pr_re_f1score_perclass[1][0]\n",
    "    f1score_true= pr_re_f1score_perclass[2][1]\n",
    "    f1score_fake= pr_re_f1score_perclass[2][0]\n",
    "\n",
    "    # AUC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_hat, pos_label=None)\n",
    "    AUC= metrics.auc(fpr, tpr)\n",
    "\n",
    "    metrices_name=['accuracy','precision_true','precision_fake','recall_true','recall_fake','f1score_true','f1score_fake', 'AUC']\n",
    "    metrices_value=[accuracy, precision_true, precision_fake, recall_true, recall_fake, f1score_true, f1score_fake, AUC]\n",
    "    i=0\n",
    "    for item in metrices_name:\n",
    "        print(item +':' ,metrices_value[i])\n",
    "        i+=1\n",
    "\n",
    "    return accuracy, precision_true, precision_fake, recall_true, recall_fake, f1score_true, f1score_fake, AUC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aqHBEyRcJR9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def scatter_plot(logits, y_t, title):\n",
    "    marker_size=20\n",
    "    figure = plt.figure(figsize=(20, 6))\n",
    "    plt.scatter(logits,logits, marker_size, c=y_t)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Predicted Probability\")\n",
    "    cbar= plt.colorbar()\n",
    "    cbar.set_label(\"Probability\", labelpad=+1)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# accuracy \n",
    "def model_evaluation(model, x_test, y_true, name):\n",
    "    y_hat= model(x_test)\n",
    "    y_pred=(np.greater(y_hat,0.505)).astype(int)\n",
    "    cm=confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    accuracy= np.trace(cm)/np.sum(cm)\n",
    "    \n",
    "    print(name+ \":\")\n",
    "  \n",
    "    print(' Accuracy:', accuracy)\n",
    "\n",
    "    # this will plot the result \n",
    "    # scatter_plot(y_hat,y_true, title=name)\n",
    "\n",
    "    return accuracy #precision, recall, f1_score, accuracy\n",
    "def plot_roc(fpr,tpr,label):\n",
    "    #  plot the roc curve for the model\n",
    "    pyplot.plot(fpr, tpr, linestyle='--', label=label)\n",
    "\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot \n",
    "    pyplot.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYaio-ylcL3b"
   },
   "source": [
    "# Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3YuD3mOcO4v"
   },
   "outputs": [],
   "source": [
    "# declaring loss function\n",
    "# ref:https://github.com/CuriousAI/mean-teacher/tree/master/tensorflow/mean_teacher  updated according to our need .\n",
    "def classification_costs(logits, labels):\n",
    "    \"\"\" Commputing classification cost , after removing labels -1 of unlabelled data and then calculating \n",
    "    the binary cross entropy .\n",
    "    \"\"\"\n",
    "    applicable = tf.not_equal(labels, -1)\n",
    "\n",
    "     # Change -1s to zeros to make cross-entropy computable\n",
    "    labels = tf.where(applicable, labels, tf.zeros_like(labels))\n",
    "\n",
    "    # This will now have incorrect values for unlabeled examples\n",
    "    per_sample = tf.keras.losses.binary_crossentropy(labels,logits)\n",
    "    # Retain costs only for labeled\n",
    "    per_sample = tf.where(applicable, per_sample, tf.zeros_like(per_sample))\n",
    "    # Take mean over all examples, not just labeled examples.\n",
    "   \n",
    "    loss = tf.math.divide( tf.reduce_mean(tf.reduce_sum(per_sample)), np.shape(per_sample)[0])\n",
    "\n",
    "    return loss\n",
    "\n",
    "#custom loss function\n",
    "def Overall_Cost(classification_cost, consistency_cost, ratio=0.5):\n",
    "    return (ratio * classification_cost) + ((1 - ratio)*consistency_cost)\n",
    "#function for consistency cost \n",
    "def Consistency_Cost(teacher_output, student_output):\n",
    "    #Kl divergence \n",
    "    # kl = tf.keras.losses.KLDivergence()\n",
    "    # sq_diff_layer=kl(teacher_output, student_output).numpy()\n",
    "    \n",
    "    #MSE\n",
    "    sq_diff_layer = tf.reduce_mean(tf.math.squared_difference(teacher_output, student_output))\n",
    "    return sq_diff_layer\n",
    "def ema(student_model, teacher_model, alpha):\n",
    "    '''\n",
    "    Calculates the exponential moving average of the student model weights and updates the teacher model weights\n",
    "    formula:\n",
    "    t_i = alpha * t_{i-1} + (1 - alpha) * s_i, with default alpha = 0.99\n",
    "    t_i = weights of teacher model in current epoch\n",
    "    s_i = weights of student model in current epoch\n",
    "    '''\n",
    "    #taking weights \n",
    "    student_weights = student_model.get_weights()\n",
    "    teacher_weights = teacher_model.get_weights()\n",
    "\n",
    "    #length must be equal otherwise it will not work \n",
    "    assert len(student_weights) == len(teacher_weights), 'length of student and teachers weights are not equal Please check. \\n Student: {}, \\n Teacher:{}'.format(\n",
    "        len(student_weights), len(teacher_weights))\n",
    "\n",
    "    new_layers = []\n",
    "    for i, layers in enumerate(student_weights):\n",
    "        new_layer = alpha*(teacher_weights[i]) + (1-alpha)*layers\n",
    "        new_layers.append(new_layer)\n",
    "    teacher_model.set_weights(new_layers)\n",
    "    return teacher_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJqtMkOXbi3s"
   },
   "source": [
    "# Writing Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIF13BPMbm3n"
   },
   "outputs": [],
   "source": [
    "def report_writing(Model,lr,Batch_Size, Epoch,Alpha,Ratio, train_accuracy,test_accuracy,precision_true,precision_fake,recall_true,recall_fake,f1score_true,f1score_fake,AUC,comment):\n",
    "    x = datetime.datetime.now()\n",
    "    report_df = pd.DataFrame(columns=['Date', 'Model','Learning Rate','Batch_Size', 'Epoch','Alpha','Ratio','Train_Accuracy',\n",
    "                                      'Test_Accuracy', 'Precision_True','Precision_Fake','Recall_True','Recall_Fake','F1_Score_True','F1_Score_Fake','AUC',\n",
    "                                      'comment'])\n",
    "    report_df = report_df.append({'Date' : x.strftime(\"%c\"), 'Model' :Model,'Learning Rate':lr,'Batch_Size' : Batch_Size, 'Epoch': Epoch,'Alpha': Alpha,'Ratio': Ratio,'Train_Accuracy': train_accuracy,\n",
    "                                  'Test_Accuracy': test_accuracy, 'Precision_True': precision_true,'Precision_Fake': precision_fake,'Recall_True': recall_true,'Recall_Fake': recall_fake,'F1_Score_True': f1score_true,'F1_Score_Fake': f1score_fake, 'AUC':AUC,'comment': comment}, ignore_index=True)\n",
    "    my_file = Path(path+report_name)\n",
    "\n",
    "    if my_file.exists():\n",
    "        report_df.to_csv(path+report_name,mode='a', header= False , index = False)\n",
    "    else:\n",
    "        report_df.to_csv(path+report_name,mode='w', header= True , index= False) \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt6qGg9JOUFI"
   },
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGh0wEpCz7wo"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_supervised(epochs, batch_size, lr, x_train, y_train, x_val, y_val,x_test, y_test,maxlen,vocab_size):\n",
    "\n",
    "    model_supervised = BiLstmModel_attention(maxlen, vocab_size)\n",
    "    model_supervised.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= lr ),loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "    print('Training supervised Model...')\n",
    "    history=model_supervised.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_val,y_val))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    \n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()  \n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # evaluation\n",
    "    train_accuracy=history.history['accuracy'][len(history.epoch)-1]\n",
    "       \n",
    "    test_accuracy,precision_true,precision_fake,recall_true,recall_fake,f1score_true,f1score_fake,AUC = prec_rec_f1score(y_test,x_test,model_supervised)\n",
    "   \n",
    "    report_writing('BILSTM_attn',lr, batch_size,len(history.epoch),'NaN','NaN', train_accuracy, \n",
    "                   test_accuracy, precision_true, precision_fake, recall_true, recall_fake, \n",
    "                   f1score_true, f1score_fake,AUC,'Supervised Learning_attention')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA3VRaeujjfn"
   },
   "source": [
    "# MEAN teacher\n",
    "In this updation takes place during each step/batch. This model doesnt work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf7EK71Fl-zo"
   },
   "outputs": [],
   "source": [
    "def train_MeanTeacher(epochs, batch_size, alpha, lr, ratio,x_train, y_train,x_val,y_val, x_test, y_test, x_unlabel_tar,vocab_size,maxlen):\n",
    "\n",
    "    #preparing the training dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    \n",
    "    #preparing the target dataset \n",
    "    tar_dataset =  tf.data.Dataset.from_tensor_slices(x_unlabel_tar)\n",
    "    tar_dataset = tar_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    #declaring optimiser\n",
    "    optimizer= tf.keras.optimizers.Adam(learning_rate= lr ) #trying changing learning rate , sometimes it gives good result \n",
    "    train_metrics = tf.keras.metrics.BinaryAccuracy(name='Binary_Accuracy')\n",
    "    val_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"Binary_Acc\")\n",
    "    teacher_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"Binary_Acc_teacher\") \n",
    "    # Creating model\n",
    "    student = BiLstmModel_attention(maxlen, vocab_size)\n",
    "    student.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= lr ),loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "    teacher = BiLstmModel_attention(maxlen, vocab_size)\n",
    "    teacher.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= lr ),loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # collecting costs\n",
    "  \n",
    "    train_accuracy=[]\n",
    "    steps=[]\n",
    "\n",
    "    #training teacher with one epoch \n",
    "   \n",
    "    #this I am doing to get all steps details in epoch\n",
    "    i=0\n",
    "    print('Train Mean teacher Model...')\n",
    "    # teacher.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n",
    "    teacher.fit(x_train,y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "    acc_t=0\n",
    "    #false positive rate and true positive rate\n",
    "    fpr=[]\n",
    "    tpr=[]\n",
    "    # x_unlabel_tar= tf.convert_to_tensor(x_unlabel_tar)\n",
    "    for epoch in range(1,epochs+1):  \n",
    "        print(*\"*****************\")\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "        print(*\"*****************\")\n",
    "        #iteration over batches \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "             with tf.GradientTape() as tape:\n",
    "         \n",
    "                # adding instant noise\n",
    "                iterator_unlabel = iter(tar_dataset)\n",
    "                x_batch_unlabel = iterator_unlabel.get_next()\n",
    "\n",
    "                '''this is one method of adding -1 label using unlable data'''\n",
    "                x_train_n,y_train_n= instant_noise(x_batch_train,y_batch_train,x_batch_unlabel,0.2)\n",
    "\n",
    "                # Run the forward pass of the layer\n",
    "                logits= student(x_train_n, training= True)  \n",
    "                # logits_acc =  student(x_batch_sn, training= False) \n",
    "\n",
    "                # doing this because then accuracy cannot be calculated\n",
    "                logits_acc= student(x_batch_train, training= True) \n",
    "\n",
    "                # calculating accuracy\n",
    "                train_metrics(y_batch_train,logits_acc)  \n",
    "\n",
    "                #Calculating classification cost \n",
    "                classification_cost = classification_costs(logits,y_train_n)\n",
    "                # classification.append(classification_cost)\n",
    "         \n",
    "                x_train_n1,_ = instant_noise(x_batch_train,y_batch_train,x_batch_unlabel,0.2)\n",
    "                           \n",
    "                tar_teacher = teacher(x_train_n1) #x_batch_train\n",
    "                #  tar_student= student(x_train_n)\n",
    "                consistency_cost= Consistency_Cost(tar_teacher,logits) \n",
    "                # consistency.append(consistency_cost)\n",
    "\n",
    "                overall_cost= Overall_Cost(classification_cost, consistency_cost, ratio=0.5)\n",
    " \n",
    "                #adding loss to student model \n",
    "             grads= tape.gradient(overall_cost, student.trainable_weights)\n",
    "             i=i+1\n",
    "             steps.append(i)\n",
    "   \n",
    "             # the value of the variables to minimize the loss.\n",
    "             optimizer.apply_gradients(zip(grads, student.trainable_weights))\n",
    "             teacher= ema(student, teacher, alpha=alpha)\n",
    "\n",
    "        train_acc = train_metrics.result()\n",
    "        print(alpha)\n",
    "   \n",
    "        #appending training accuracy\n",
    "        train_accuracy.append(train_acc)\n",
    "\n",
    "       \n",
    "            # Reset training metrics at the end of each epoch\n",
    "        train_metrics.reset_states()\n",
    "   \n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        print('*******STUDENT*************')\n",
    "        prec_rec_f1score(y_val,x_val,student)\n",
    "        print('*******TEACHER*************')\n",
    "        prec_rec_f1score(y_val,x_val,teacher)\n",
    "\n",
    "   \n",
    "        if epoch >= 10 and epoch % 5 ==0 :\n",
    "            print('---------------------------STUDENT--------------------------')\n",
    "            test_accuracy,precision_true,precision_fake,recall_true,recall_fake,f1score_true,f1score_fake,AUC = prec_rec_f1score(y_test,x_test,student)\n",
    "            report_writing('Student',lr, batch_size,epoch,alpha,ratio, train_acc.numpy(), \n",
    "                           test_accuracy, precision_true, precision_fake, recall_true, recall_fake, \n",
    "                           f1score_true, f1score_fake,AUC,'MeanTeacher-Attention')  \n",
    "            print('-----------------------------------------------------------------')\n",
    "    \n",
    "            print('---------------------------TEACHER---------------------------------')\n",
    "       \n",
    "            test_accuracy,precision_true,precision_fake,recall_true,recall_fake,f1score_true,f1score_fake,AUC = prec_rec_f1score(y_test,x_test,teacher)\n",
    "            report_writing('Teacher',lr, batch_size,epoch,alpha,ratio, train_acc.numpy(), \n",
    "                           test_accuracy, precision_true, precision_fake, recall_true, recall_fake, \n",
    "                           f1score_true, f1score_fake,AUC,'MeanTeacher-Attention') \n",
    "       \n",
    "            print('-----------------------------------------------------------------')\n",
    "        # if epoch >= 10 and epoch% 5==0 :\n",
    "        #     teacher.save(path+'/model/MT_unlabel_'+str(epoch))    \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    return teacher\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktWjsTuP2DOQ"
   },
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "het--_x6MaOz"
   },
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq6ZEe6bO9n6"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "lr=0.0001\n",
    "\n",
    "batch_size= 64\n",
    "#for mean teacher \n",
    "ratio =0.5\n",
    "alpha=0.99 #(0.90-0.99)\n",
    "maxlen=510"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xeqi_OTdMZQ0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "am6rr0RmMdfP"
   },
   "source": [
    "# BiLSTM-Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCmiDBVgtFwg"
   },
   "outputs": [],
   "source": [
    "\n",
    "# report_name='report/BiLSTM-Attn.csv'\n",
    "# for i in range(5):\n",
    "#     x_train= np.load(path+'train_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "#     y_train= np.load(path+'train_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "#     x_val= np.load(path+'dev_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "#     y_val= np.load(path+'dev_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "#     x_test= np.load(path+'test_x.npy', allow_pickle=True)\n",
    "#     y_test= np.load(path+'test_y.npy', allow_pickle=True)\n",
    "\n",
    "#     print('train data size:', np.shape(x_train))\n",
    "#     print('train data True/Fake count:', np.count_nonzero(y_train), len(y_train)- np.count_nonzero(y_train))\n",
    "#     print('val data size:', np.shape(x_val))\n",
    "#     print('val data True/Fake count:', np.count_nonzero(y_val), len(y_val)- np.count_nonzero(y_val))\n",
    "\n",
    "#     print('test data size:', np.shape(x_test))\n",
    "#     print('test data True/Fake count:', np.count_nonzero(y_test), len(y_test)- np.count_nonzero(y_test))\n",
    "\n",
    "#     x_unlabel= np.load(path+'unlabeled_'+'all'+'_x.npy', allow_pickle=True)\n",
    "#     x_train,x_val, x_test, x_unlabel, vocab_size, tokenizer= tokenization(x_train ,x_val,  x_test, x_unlabel, maxlen)\n",
    "#     print(vocab_size)\n",
    "\n",
    "#     epochs=10\n",
    "#     model = train_supervised(epochs, batch_size, lr, x_train, y_train, x_val, y_val,x_test, y_test,maxlen,vocab_size)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCA4rw8QMmv1"
   },
   "source": [
    "# BiLSTM-Attn-MT-Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWiAvDjCMli-",
    "outputId": "a106b19b-62f9-4204-b853-73c095c22a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------FOLD Number:--------------- 0\n",
      "train data size: (1256,)\n",
      "train data True/Fake count: 377 879\n",
      "val data size: (316,)\n",
      "test data size: (394,)\n",
      "train data True/Fake count: 158 236\n",
      "Train Mean teacher Model...\n",
      "20/20 [==============================] - 60s 3s/step - loss: 0.6395 - accuracy: 0.6712\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 1\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.4864864864864865\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.5809712151175566\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 2\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.6735223027905954\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.5027905954735223\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 3\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6234177215189873\n",
      "precision_true: 0.25\n",
      "precision_fake: 0.6433333333333333\n",
      "recall_true: 0.036036036036036036\n",
      "recall_fake: 0.9414634146341463\n",
      "f1score_true: 0.06299212598425197\n",
      "f1score_fake: 0.7643564356435644\n",
      "AUC: 0.6850362557679631\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.4850802021533729\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 4\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6550632911392406\n",
      "precision_true: 0.5833333333333334\n",
      "precision_fake: 0.6578947368421053\n",
      "recall_true: 0.06306306306306306\n",
      "recall_fake: 0.975609756097561\n",
      "f1score_true: 0.1138211382113821\n",
      "f1score_fake: 0.7858546168958743\n",
      "AUC: 0.6922434629751704\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.5174247418149858\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 5\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6613924050632911\n",
      "precision_true: 0.5666666666666667\n",
      "precision_fake: 0.6713286713286714\n",
      "recall_true: 0.15315315315315314\n",
      "recall_fake: 0.9365853658536586\n",
      "f1score_true: 0.24113475177304963\n",
      "f1score_fake: 0.7820773930753565\n",
      "AUC: 0.7024390243902439\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.5862008349813228\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 6\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6930379746835443\n",
      "precision_true: 0.5921052631578947\n",
      "precision_fake: 0.725\n",
      "recall_true: 0.40540540540540543\n",
      "recall_fake: 0.848780487804878\n",
      "f1score_true: 0.48128342245989303\n",
      "f1score_fake: 0.7820224719101124\n",
      "AUC: 0.7109646231597452\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6487341772151899\n",
      "precision_true: 0.0\n",
      "precision_fake: 0.6487341772151899\n",
      "recall_true: 0.0\n",
      "recall_fake: 1.0\n",
      "f1score_true: 0.0\n",
      "f1score_fake: 0.7869481765834934\n",
      "AUC: 0.6342342342342342\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 7\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6867088607594937\n",
      "precision_true: 0.5909090909090909\n",
      "precision_fake: 0.712\n",
      "recall_true: 0.35135135135135137\n",
      "recall_fake: 0.8682926829268293\n",
      "f1score_true: 0.4406779661016949\n",
      "f1score_fake: 0.7824175824175823\n",
      "AUC: 0.7103493737640079\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6518987341772152\n",
      "precision_true: 0.5555555555555556\n",
      "precision_fake: 0.6547231270358306\n",
      "recall_true: 0.04504504504504504\n",
      "recall_fake: 0.9804878048780488\n",
      "f1score_true: 0.08333333333333334\n",
      "f1score_fake: 0.7851562500000001\n",
      "AUC: 0.668138870577895\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 8\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.6740506329113924\n",
      "precision_true: 0.6818181818181818\n",
      "precision_fake: 0.673469387755102\n",
      "recall_true: 0.13513513513513514\n",
      "recall_fake: 0.9658536585365853\n",
      "f1score_true: 0.22556390977443608\n",
      "f1score_fake: 0.7935871743486973\n",
      "AUC: 0.7199736321687541\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6677215189873418\n",
      "precision_true: 0.65\n",
      "precision_fake: 0.668918918918919\n",
      "recall_true: 0.11711711711711711\n",
      "recall_fake: 0.9658536585365853\n",
      "f1score_true: 0.1984732824427481\n",
      "f1score_fake: 0.7904191616766468\n",
      "AUC: 0.6895627334651725\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 9\n",
      "* * * * * * * * * * * * * * * * *\n",
      "0.99\n",
      "*******STUDENT*************\n",
      "accuracy: 0.7025316455696202\n",
      "precision_true: 0.717948717948718\n",
      "precision_fake: 0.7003610108303249\n",
      "recall_true: 0.25225225225225223\n",
      "recall_fake: 0.9463414634146341\n",
      "f1score_true: 0.3733333333333333\n",
      "f1score_fake: 0.8049792531120332\n",
      "AUC: 0.7229619863766206\n",
      "*******TEACHER*************\n",
      "accuracy: 0.6613924050632911\n",
      "precision_true: 0.5833333333333334\n",
      "precision_fake: 0.6678082191780822\n",
      "recall_true: 0.12612612612612611\n",
      "recall_fake: 0.9512195121951219\n",
      "f1score_true: 0.2074074074074074\n",
      "f1score_fake: 0.7847082494969819\n",
      "AUC: 0.7007690617446716\n",
      "* * * * * * * * * * * * * * * * *\n",
      "Start of epoch 10\n",
      "* * * * * * * * * * * * * * * * *\n"
     ]
    }
   ],
   "source": [
    "report_name='report/BiLSTM-Attn-MT-Mix.csv'\n",
    "for i in range(1):\n",
    "    print('-------------FOLD Number:---------------',i)\n",
    "    x_train= np.load(path+'train_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    y_train= np.load(path+'train_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "    x_val= np.load(path+'dev_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    y_val= np.load(path+'dev_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "    x_test= np.load(path+'test_x.npy', allow_pickle=True)\n",
    "    y_test= np.load(path+'test_y.npy', allow_pickle=True)\n",
    "    x_unlabel= np.load(path+'unlabeled_'+'mix'+'_x.npy', allow_pickle=True)\n",
    "\n",
    "    print('train data size:', np.shape(x_train))\n",
    "    print('train data True/Fake count:', np.count_nonzero(y_train), len(y_train)- np.count_nonzero(y_train))\n",
    "    print('val data size:', np.shape(x_val))\n",
    "\n",
    "    print('test data size:', np.shape(x_test))\n",
    "    print('train data True/Fake count:', np.count_nonzero(y_test), len(y_test)- np.count_nonzero(y_test))\n",
    "\n",
    "    x_train,x_val, x_test, x_unlabel, vocab_size, tokenizer= tokenization(x_train ,x_val,  x_test, x_unlabel, maxlen)\n",
    "    epochs=20\n",
    "    model = train_MeanTeacher(epochs, batch_size, alpha, lr, ratio,x_train, y_train,x_val,y_val, x_test, y_test, x_unlabel,vocab_size,maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oujY0QWUMo8t"
   },
   "source": [
    "# BiLSTM-Attn-MT-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjWq8MJvMssF"
   },
   "outputs": [],
   "source": [
    "report_name='report/BiLSTM-Attn-MT-All.csv'\n",
    "for i in range(5):\n",
    "    x_train= np.load(path+'train_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    y_train= np.load(path+'train_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "    x_val= np.load(path+'dev_'+str(i)+'_x.npy', allow_pickle=True)\n",
    "    y_val= np.load(path+'dev_'+str(i)+'_y.npy', allow_pickle=True)\n",
    "\n",
    "    x_test= np.load(path+'test_x.npy', allow_pickle=True)\n",
    "    y_test= np.load(path+'test_y.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "    x_unlabel= np.load(path+'unlabeled_'+'all'+'_x.npy', allow_pickle=True)\n",
    "    print('train data size:', np.shape(x_train))\n",
    "    print('train data True/Fake count:', np.count_nonzero(y_train), len(y_train)- np.count_nonzero(y_train))\n",
    "    print('val data size:', np.shape(x_val))\n",
    "\n",
    "    print('test data size:', np.shape(x_test))\n",
    "    print('train data True/Fake count:', np.count_nonzero(y_test), len(y_test)- np.count_nonzero(y_test))\n",
    "\n",
    "    x_train,x_val, x_test, x_unlabel, vocab_size, tokenizer= tokenization(x_train ,x_val,  x_test, x_unlabel, maxlen)\n",
    "\n",
    "    epochs=15\n",
    "    model = train_MeanTeacher(epochs, batch_size, alpha, lr, ratio,x_train, y_train,x_val,y_val, x_test, y_test, x_unlabel,vocab_size,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZPZa0CeWpOj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AozvbK25RZ6m",
    "swtc3CtsvXH5",
    "CrpTHBDBb4Uv",
    "zYaio-ylcL3b",
    "Xt6qGg9JOUFI"
   ],
   "include_colab_link": true,
   "name": "BiLSTM-Attn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
